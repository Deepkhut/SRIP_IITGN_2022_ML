{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TASK3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOnwB+4qOgWooOuIPXeEQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepkhut/SRIP_IITGN_2022_ML/blob/main/TASK3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xi4_dIJ0A8_T"
      },
      "outputs": [],
      "source": [
        "##### download MNIST data and store it in under directory _DATA\n",
        "\n",
        "import array\n",
        "import gzip\n",
        "import os\n",
        "from os import path\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# path to data directory\n",
        "_DATA = \"/tmp/jax_example_data/\"\n",
        "\n",
        "\n",
        "def _download(url, filename):\n",
        "    \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n",
        "    if not path.exists(_DATA):\n",
        "        os.makedirs(_DATA)\n",
        "    out_file = path.join(_DATA, filename)\n",
        "    if not path.isfile(out_file):\n",
        "        urllib.request.urlretrieve(url, out_file)\n",
        "        print(\"downloaded {} to {}\".format(url, _DATA))\n",
        "\n",
        "\n",
        "def _partial_flatten(x):\n",
        "    \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n",
        "    return np.reshape(x, (x.shape[0], -1))\n",
        "\n",
        "\n",
        "def _one_hot(x, k, dtype=np.float32):\n",
        "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "    return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "\n",
        "def mnist_raw():\n",
        "    \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n",
        "    # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
        "    base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
        "\n",
        "    def parse_labels(filename):\n",
        "        with gzip.open(filename, \"rb\") as fh:\n",
        "            _ = struct.unpack(\">II\", fh.read(8))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "    def parse_images(filename):\n",
        "        with gzip.open(filename, \"rb\") as fh:\n",
        "            _, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "            return np.array(array.array(\"B\", fh.read()),\n",
        "                          dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "    for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n",
        "        _download(base_url + filename, filename)\n",
        "\n",
        "    train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n",
        "    train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n",
        "    test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n",
        "    test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "def mnist(permute_train=False):\n",
        "    \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n",
        "    train_images, train_labels, test_images, test_labels = mnist_raw()\n",
        "\n",
        "    train_images = _partial_flatten(train_images) / np.float32(255.)\n",
        "    test_images = _partial_flatten(test_images) / np.float32(255.)\n",
        "    train_labels = _one_hot(train_labels, 10)\n",
        "    test_labels = _one_hot(test_labels, 10)\n",
        "\n",
        "    if permute_train:\n",
        "        perm = np.random.RandomState(0).permutation(train_images.shape[0])\n",
        "        train_images = train_images[perm]\n",
        "        train_labels = train_labels[perm]\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_generator():\n",
        "    #while True:\n",
        "        for j in range(10):\n",
        "            yield j\n",
        "            \n",
        "gen = my_generator()\n",
        "print(gen) # shows a generator object\n",
        "\n",
        "print('\\nfirst loop:\\n')   \n",
        "\n",
        "# call generator\n",
        "for i in range(4):\n",
        "    print(i, next(gen) )\n",
        "    \n",
        "print('\\nsecond loop:\\n')    \n",
        "    \n",
        "for i in range(10):\n",
        "    print(i, next(gen) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "MVzx_jDVBz5F",
        "outputId": "0f9159cc-8b41-4760-ee87-4b35bd4661cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object my_generator at 0x7f9e5d538a50>\n",
            "\n",
            "first loop:\n",
            "\n",
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "\n",
            "second loop:\n",
            "\n",
            "0 4\n",
            "1 5\n",
            "2 6\n",
            "3 7\n",
            "4 8\n",
            "5 9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e0008476830e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### define minibatches\n",
        "\n",
        "# fix seed\n",
        "seed=0\n",
        "np.random.seed(seed)\n",
        "\n",
        "##### define data variables and the minibatch generator\n",
        "# load MNIST data\n",
        "train_images, train_labels, test_images, test_labels = mnist()\n",
        "\n",
        "print('\\ntrain data: image shape: {}, label shape: {}.'.format(train_images.shape, train_labels.shape ))\n",
        "print('test data : image shape: {}, label shape: {}.\\n'.format(test_images.shape, test_labels.shape ))\n",
        "\n",
        "\n",
        "# size of a single minibatch\n",
        "batch_size=128 \n",
        "# size of the trining set\n",
        "num_train = train_images.shape[0] \n",
        "# define number of complete minibatches (data size need not be muptiple of batch_size)\n",
        "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
        "# total number of minibatches is the smallest integer to fit all minibatches in the dataset\n",
        "num_batches = num_complete_batches + bool(leftover)\n",
        "\n",
        "\n",
        "def data_stream():\n",
        "    \"\"\"\n",
        "    This function defines a generator which produces random batches of data, one at a time.\n",
        "    \n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(0)\n",
        "    while True:\n",
        "        perm = rng.permutation(num_train) # compute a random permutation\n",
        "        for i in range(num_batches):\n",
        "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "            yield train_images[batch_idx], train_labels[batch_idx]\n",
        "\n",
        "# define the batches generator\n",
        "batches = data_stream()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ztExmVWB1pL",
        "outputId": "8df6c5a4-6e28-42ea-e809-ba14045974e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train data: image shape: (60000, 784), label shape: (60000, 10).\n",
            "test data : image shape: (10000, 784), label shape: (10000, 10).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "# static plots\n",
        "%matplotlib inline \n",
        "\n",
        "### show the first data point as an example\n",
        "n=1111 # test data point number\n",
        "\n",
        "plt.imshow(test_images[n].reshape(28,28),cmap='Greys')\n",
        "plt.title('label: {}'.format(test_labels[n]) )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "KNEELbLcCfFM",
        "outputId": "8f76638e-8e49-4756-d670-d5131e66b134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define CNN model"
      ],
      "metadata": {
        "id": "O0bHhIRnEtd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp   # jax's numpy version with GPU support\n",
        "from jax import random    # used to define a RNG key to control the random input in JAX\n",
        "from jax.experimental import stax # neural network library\n",
        "from jax.experimental.stax import Dense, Relu, LogSoftmax # neural network layers\n",
        "from jax.experimental.stax import GeneralConv, Flatten # neural network layers\n",
        "\n",
        "rng = random.PRNGKey(seed)\n",
        "# cast data into 2D image format\n",
        "train_images = train_images.reshape(-1,1,28,28) # -1: number of data points, 1: input channels, (28,28) = (height, width) dimensions of image\n",
        "test_images = test_images.reshape(-1,1,28,28)\n",
        "\n",
        "# conv net convention\n",
        "dim_nums=('NCHW', 'OIHW', 'NCHW') # default for (input, filters, output)\n",
        "\n",
        "# define functions which initialize the parameters and evaluate the model\n",
        "initialize_params, predict = stax.serial(    \n",
        "                                            ### convolutional NN (CNN)\n",
        "                                            GeneralConv(dim_nums, 16, (4,4), strides=(4,4) ), # 16 output channels, (4,4) filter\n",
        "                                            Relu,\n",
        "                                            GeneralConv(dim_nums, 32, (3,3), strides=(1,1) ), # 32 output channels, (3,3) filter\n",
        "                                            Relu,\n",
        "                                            Flatten, # flatten output\n",
        "                                            Dense(256), # 256 hidden neurons\n",
        "                                            Relu,\n",
        "                                            Dense(10), # 10 output neurons\n",
        "                                            LogSoftmax # NB: computes the log-probability\n",
        "                                        )\n",
        "\n",
        "# initialize the model parameters\n",
        "output_shape, inital_params = initialize_params(rng, (-1, 1, 28, 28)) # conv layer, 1 input channel, 28x28 pixes in each image\n",
        "\n",
        "print('\\noutput shape of the model is {}.\\n'.format(output_shape))\n",
        "\n",
        "# check how network works on 3 examples\n",
        "predictions = predict(inital_params, test_images[0:3])\n",
        "\n",
        "# print shape of output\n",
        "print(\"actual output shape is:\", predictions.shape)\n",
        "\n",
        "# check if probability is conserved\n",
        "print('log(softmax) values:', predictions[0])\n",
        "print('conservation of probability', np.sum(jnp.exp(predictions), axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02qHErJVEx8Q",
        "outputId": "494462b2-3147-459a-b58a-45cd437bed60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/stax.py:30: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead\n",
            "  FutureWarning)\n",
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "output shape of the model is (-1, 10).\n",
            "\n",
            "actual output shape is: (3, 10)\n",
            "log(softmax) values: [-2.264744  -2.4081535 -2.2366064 -2.302778  -2.4543524 -2.3551545\n",
            " -2.2510574 -2.1797054 -2.268157  -2.3361628]\n",
            "conservation of probability [1.0000001 1.0000001 1.0000001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Loss/cost function"
      ],
      "metadata": {
        "id": "tIH2-GNOGFvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### define loss and accuracy functions\n",
        "\n",
        "from jax import grad\n",
        "from jax.tree_util import tree_flatten # jax params are stored as nested tuples; useing this to manipulate tuples\n",
        "\n",
        "\n",
        "def l2_regularizer(params, lmbda):\n",
        "    \"\"\"\n",
        "    Define l2 regularizer: $\\lambda \\ sum_j ||theta_j||^2 $ for every parameter in the model $\\theta_j$\n",
        "    \n",
        "    \"\"\"\n",
        "    return lmbda*jnp.sum(jnp.array([jnp.sum(jnp.abs(theta)**2) for theta in tree_flatten(params)[0] ]))\n",
        "\n",
        "\n",
        "def loss(params, batch):\n",
        "    \"\"\"\n",
        "    Define cost (or lost) function for softmax classification. \n",
        "    \n",
        "    \"\"\"\n",
        "    inputs, targets = batch\n",
        "    preds = predict(params, inputs)\n",
        "    return -jnp.mean(jnp.sum(preds * targets, axis=1)) + l2_regularizer(params, 0.001)\n",
        "\n",
        "\n",
        "def mean_accuracy(params, batch):\n",
        "    \"\"\"\n",
        "    Define accuracy function: the mean number of datapoints which have correct preductions. \n",
        "    This function is not used for training; only to test the performance. \n",
        "    \n",
        "    \"\"\"\n",
        "    inputs, targets = batch\n",
        "    target_class = jnp.argmax(targets, axis=1)\n",
        "    predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
        "    return jnp.mean(predicted_class == target_class)"
      ],
      "metadata": {
        "id": "k5AcdqmlF9IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the optimizer"
      ],
      "metadata": {
        "id": "DLOaAnyeGUXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### define generalized gradient descent optimizer and a function to update model parameters\n",
        "\n",
        "from jax.experimental import optimizers # gradient descent optimizers\n",
        "from jax import jit\n",
        "\n",
        "step_size = 0.001 # step size or learning rate \n",
        "momentum_mass = 0.9 # \"gamma\" parameter in GD+momentum\n",
        "\n",
        "# compute optimizer functions\n",
        "opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)\n",
        "\n",
        "\n",
        "# define function which updates the parameters using the change computed by the optimizer\n",
        "@jit # Just In Time compilation speeds up the code; requires to use jnp everywhere; remove when debugging\n",
        "def update(i, opt_state, batch):\n",
        "    \"\"\"\n",
        "    i: int,\n",
        "        counter to count how many update steps we have performed\n",
        "    opt_state: object,\n",
        "        the state of the optimizer\n",
        "    batch: np.array\n",
        "        batch containing the data used to update the model\n",
        "        \n",
        "    Returns: \n",
        "    opt_state: object,\n",
        "        the new state of the optimizer\n",
        "        \n",
        "    \"\"\"\n",
        "    # get current parameters of the model\n",
        "    current_params = get_params(opt_state)\n",
        "    # compute gradients\n",
        "    grad_params = grad(loss)(current_params, batch)\n",
        "    # use the optimizer to perform the update using opt_update\n",
        "    return opt_update(i, grad_params, opt_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqE-rPawGRKB",
        "outputId": "e163218a-c1c3-4ec9-ae94-1d9cd70e7a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/optimizers.py:30: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
            "  FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "67MY4cWjGseP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import itertools\n",
        "\n",
        "# define geenrator to count the number of updates\n",
        "itercount = itertools.count()\n",
        "\n",
        "# define number of training epochs\n",
        "num_epochs = 101\n",
        "\n",
        "# define figures of merit\n",
        "train_accuracy=np.zeros(num_epochs)\n",
        "test_accuracy=np.zeros_like(train_accuracy)\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "\n",
        "# set the initial model parameters in the optimizer\n",
        "opt_state = opt_init(inital_params)\n",
        "\n",
        "# loop over the number of training epochs\n",
        "for epoch in range(num_epochs): \n",
        "    \n",
        "    ### record time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    ### train in minibatches until the entire dataset is exhausted: \n",
        "    # the entire dataset is divided into _random_ minibatches; \n",
        "    # all minibatches are shown to the model before going to next epoch\n",
        "    for _ in range(num_batches):\n",
        "        # use the data to update the model parameters\n",
        "        opt_state = update(next(itercount), opt_state, next(batches))\n",
        "        \n",
        "    ### record time needed for a single epoch\n",
        "    epoch_time = time.time() - start_time\n",
        "    \n",
        "    ### evaluate performance of the model at each fixed epoch\n",
        "    \n",
        "    # retrieve current model parameters\n",
        "    params = get_params(opt_state)\n",
        "    \n",
        "    # measure the accuracy on the training and test datasets\n",
        "    train_accuracy[epoch] = mean_accuracy(params, (train_images, train_labels))\n",
        "    test_accuracy[epoch] = mean_accuracy(params, (test_images, test_labels))\n",
        "    \n",
        "    # print results every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "        print(\"Training set accuracy {}\".format(train_accuracy[epoch]))\n",
        "        print(\"Test set accuracy {}\\n\".format(test_accuracy[epoch]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyM0dNvPGrIl",
        "outputId": "7b571cd1-90b0-4b08-f098-f6b9f1d93346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 0 in 9.64 sec\n",
            "Training set accuracy 0.8501333594322205\n",
            "Test set accuracy 0.8628999590873718\n",
            "\n",
            "Epoch 10 in 9.73 sec\n",
            "Training set accuracy 0.9580000042915344\n",
            "Test set accuracy 0.9599999785423279\n",
            "\n",
            "Epoch 20 in 7.85 sec\n",
            "Training set accuracy 0.972516655921936\n",
            "Test set accuracy 0.9708999991416931\n",
            "\n",
            "Epoch 30 in 7.97 sec\n",
            "Training set accuracy 0.9777833223342896\n",
            "Test set accuracy 0.9739999771118164\n",
            "\n",
            "Epoch 40 in 7.88 sec\n",
            "Training set accuracy 0.9822500348091125\n",
            "Test set accuracy 0.9771999716758728\n",
            "\n",
            "Epoch 50 in 7.92 sec\n",
            "Training set accuracy 0.9850333333015442\n",
            "Test set accuracy 0.9800999760627747\n",
            "\n",
            "Epoch 60 in 7.87 sec\n",
            "Training set accuracy 0.9858666658401489\n",
            "Test set accuracy 0.9789999723434448\n",
            "\n",
            "Epoch 70 in 7.72 sec\n",
            "Training set accuracy 0.9880833625793457\n",
            "Test set accuracy 0.9803999662399292\n",
            "\n",
            "Epoch 80 in 8.09 sec\n",
            "Training set accuracy 0.9890833497047424\n",
            "Test set accuracy 0.9818999767303467\n",
            "\n",
            "Epoch 90 in 8.02 sec\n",
            "Training set accuracy 0.989549994468689\n",
            "Test set accuracy 0.9817999601364136\n",
            "\n",
            "Epoch 100 in 8.06 sec\n",
            "Training set accuracy 0.9898499846458435\n",
            "Test set accuracy 0.9818999767303467\n",
            "\n"
          ]
        }
      ]
    }
  ]
}